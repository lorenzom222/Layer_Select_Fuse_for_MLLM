[2025-04-05 02:27:38,092] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-05 02:27:42,486] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-05 02:27:42,486] [INFO] [runner.py:605:main] cmd = /opt/conda/envs/test/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train.py --deepspeed ./scripts/zero2.json --model_name_or_path mtgv/MobileLLaMA-1.4B-Base --version plain --data_path ./playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json --image_folder ./playground/data/LLaVA-Pretrain/images --vision_tower google/siglip-so400m-patch14-384 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --layer_using_strategy 3-18-23 --layer_fusing_strategy I_D --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir ./checkpoint/test-I_D-pretrain-3-18-23-siglip_14_665k --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 500 --max_steps 10 --save_total_limit 4 --learning_rate 1e-3 --weight_decay 5e-2 --warmup_steps 200 --lr_scheduler_type cosine --logging_steps 1 --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --wandb_name test-I_D-pretrain-3-18-23-siglip_14_665k
[2025-04-05 02:27:49,911] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-05 02:27:54,256] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2025-04-05 02:27:54,256] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2025-04-05 02:27:54,256] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2025-04-05 02:27:54,256] [INFO] [launch.py:164:main] dist_world_size=8
[2025-04-05 02:27:54,256] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2025-04-05 02:27:54,258] [INFO] [launch.py:256:main] process 249612 spawned with command: ['/opt/conda/envs/test/bin/python3', '-u', 'llava/train/train.py', '--local_rank=0', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', 'mtgv/MobileLLaMA-1.4B-Base', '--version', 'plain', '--data_path', './playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json', '--image_folder', './playground/data/LLaVA-Pretrain/images', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--layer_using_strategy', '3-18-23', '--layer_fusing_strategy', 'I_D', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './checkpoint/test-I_D-pretrain-3-18-23-siglip_14_665k', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--max_steps', '10', '--save_total_limit', '4', '--learning_rate', '1e-3', '--weight_decay', '5e-2', '--warmup_steps', '200', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--wandb_name', 'test-I_D-pretrain-3-18-23-siglip_14_665k']
[2025-04-05 02:27:54,258] [INFO] [launch.py:256:main] process 249613 spawned with command: ['/opt/conda/envs/test/bin/python3', '-u', 'llava/train/train.py', '--local_rank=1', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', 'mtgv/MobileLLaMA-1.4B-Base', '--version', 'plain', '--data_path', './playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json', '--image_folder', './playground/data/LLaVA-Pretrain/images', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--layer_using_strategy', '3-18-23', '--layer_fusing_strategy', 'I_D', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './checkpoint/test-I_D-pretrain-3-18-23-siglip_14_665k', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--max_steps', '10', '--save_total_limit', '4', '--learning_rate', '1e-3', '--weight_decay', '5e-2', '--warmup_steps', '200', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--wandb_name', 'test-I_D-pretrain-3-18-23-siglip_14_665k']
[2025-04-05 02:27:54,260] [INFO] [launch.py:256:main] process 249614 spawned with command: ['/opt/conda/envs/test/bin/python3', '-u', 'llava/train/train.py', '--local_rank=2', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', 'mtgv/MobileLLaMA-1.4B-Base', '--version', 'plain', '--data_path', './playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json', '--image_folder', './playground/data/LLaVA-Pretrain/images', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--layer_using_strategy', '3-18-23', '--layer_fusing_strategy', 'I_D', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './checkpoint/test-I_D-pretrain-3-18-23-siglip_14_665k', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--max_steps', '10', '--save_total_limit', '4', '--learning_rate', '1e-3', '--weight_decay', '5e-2', '--warmup_steps', '200', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--wandb_name', 'test-I_D-pretrain-3-18-23-siglip_14_665k']
[2025-04-05 02:27:54,261] [INFO] [launch.py:256:main] process 249615 spawned with command: ['/opt/conda/envs/test/bin/python3', '-u', 'llava/train/train.py', '--local_rank=3', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', 'mtgv/MobileLLaMA-1.4B-Base', '--version', 'plain', '--data_path', './playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json', '--image_folder', './playground/data/LLaVA-Pretrain/images', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--layer_using_strategy', '3-18-23', '--layer_fusing_strategy', 'I_D', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './checkpoint/test-I_D-pretrain-3-18-23-siglip_14_665k', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--max_steps', '10', '--save_total_limit', '4', '--learning_rate', '1e-3', '--weight_decay', '5e-2', '--warmup_steps', '200', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--wandb_name', 'test-I_D-pretrain-3-18-23-siglip_14_665k']
[2025-04-05 02:27:54,262] [INFO] [launch.py:256:main] process 249616 spawned with command: ['/opt/conda/envs/test/bin/python3', '-u', 'llava/train/train.py', '--local_rank=4', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', 'mtgv/MobileLLaMA-1.4B-Base', '--version', 'plain', '--data_path', './playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json', '--image_folder', './playground/data/LLaVA-Pretrain/images', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--layer_using_strategy', '3-18-23', '--layer_fusing_strategy', 'I_D', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './checkpoint/test-I_D-pretrain-3-18-23-siglip_14_665k', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--max_steps', '10', '--save_total_limit', '4', '--learning_rate', '1e-3', '--weight_decay', '5e-2', '--warmup_steps', '200', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--wandb_name', 'test-I_D-pretrain-3-18-23-siglip_14_665k']
[2025-04-05 02:27:54,263] [INFO] [launch.py:256:main] process 249617 spawned with command: ['/opt/conda/envs/test/bin/python3', '-u', 'llava/train/train.py', '--local_rank=5', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', 'mtgv/MobileLLaMA-1.4B-Base', '--version', 'plain', '--data_path', './playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json', '--image_folder', './playground/data/LLaVA-Pretrain/images', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--layer_using_strategy', '3-18-23', '--layer_fusing_strategy', 'I_D', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './checkpoint/test-I_D-pretrain-3-18-23-siglip_14_665k', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--max_steps', '10', '--save_total_limit', '4', '--learning_rate', '1e-3', '--weight_decay', '5e-2', '--warmup_steps', '200', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--wandb_name', 'test-I_D-pretrain-3-18-23-siglip_14_665k']
[2025-04-05 02:27:54,264] [INFO] [launch.py:256:main] process 249618 spawned with command: ['/opt/conda/envs/test/bin/python3', '-u', 'llava/train/train.py', '--local_rank=6', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', 'mtgv/MobileLLaMA-1.4B-Base', '--version', 'plain', '--data_path', './playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json', '--image_folder', './playground/data/LLaVA-Pretrain/images', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--layer_using_strategy', '3-18-23', '--layer_fusing_strategy', 'I_D', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './checkpoint/test-I_D-pretrain-3-18-23-siglip_14_665k', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--max_steps', '10', '--save_total_limit', '4', '--learning_rate', '1e-3', '--weight_decay', '5e-2', '--warmup_steps', '200', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--wandb_name', 'test-I_D-pretrain-3-18-23-siglip_14_665k']
[2025-04-05 02:27:54,265] [INFO] [launch.py:256:main] process 249619 spawned with command: ['/opt/conda/envs/test/bin/python3', '-u', 'llava/train/train.py', '--local_rank=7', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', 'mtgv/MobileLLaMA-1.4B-Base', '--version', 'plain', '--data_path', './playground/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json', '--image_folder', './playground/data/LLaVA-Pretrain/images', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--layer_using_strategy', '3-18-23', '--layer_fusing_strategy', 'I_D', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './checkpoint/test-I_D-pretrain-3-18-23-siglip_14_665k', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--max_steps', '10', '--save_total_limit', '4', '--learning_rate', '1e-3', '--weight_decay', '5e-2', '--warmup_steps', '200', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--wandb_name', 'test-I_D-pretrain-3-18-23-siglip_14_665k']
[2025-04-05 02:28:10,557] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-05 02:28:10,560] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-05 02:28:10,628] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-05 02:28:10,720] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-05 02:28:10,774] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-05 02:28:10,775] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-05 02:28:10,827] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-05 02:28:10,861] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-05 02:28:13,037] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-05 02:28:13,081] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-05 02:28:13,268] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-05 02:28:13,268] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-05 02:28:13,294] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-05 02:28:13,316] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-05 02:28:13,346] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-05 02:28:13,385] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-05 02:28:13,460] [INFO] [comm.py:658:init_distributed] cdb=None
----------------------set wandb task name as: test-I_D-pretrain-3-18-23-siglip_14_665k
 model.config.mm_use_im_start_end: False
 model.config.mm_projector_lr: None
training_args.use_im_start_end: False
model.config.tokenizer_model_max_length: 2048
model.config.mm_use_im_patch_token: False
model.config.tokenizer_padding_side: right
model.config.image_aspect_ratio: square
model_args.version: plain
tokenizer.pad_token: <unk>
mm_patch_merge_type: flat
 model.config.mm_use_im_start_end: False
 model.config.mm_projector_lr: None
training_args.use_im_start_end: False
model.config.tokenizer_model_max_length: 2048
model.config.mm_use_im_patch_token: False
model.config.tokenizer_padding_side: right
model.config.image_aspect_ratio: square
model_args.version: plain
tokenizer.pad_token: <unk>
mm_patch_merge_type: flat
 model.config.mm_use_im_start_end: False
 model.config.mm_projector_lr: None
training_args.use_im_start_end: False
model.config.tokenizer_model_max_length: 2048
model.config.mm_use_im_patch_token: False
model.config.tokenizer_padding_side: right
model.config.image_aspect_ratio: square
model_args.version: plain
tokenizer.pad_token: <unk>
mm_patch_merge_type: flat
 model.config.mm_use_im_start_end: False
 model.config.mm_projector_lr: None
training_args.use_im_start_end: False
model.config.tokenizer_model_max_length: 2048
model.config.mm_use_im_patch_token: False
model.config.tokenizer_padding_side: right
model.config.image_aspect_ratio: square
model_args.version: plain
tokenizer.pad_token: <unk>
mm_patch_merge_type: flat
 model.config.mm_use_im_start_end: False
 model.config.mm_projector_lr: None
training_args.use_im_start_end: False
model.config.tokenizer_model_max_length: 2048
model.config.mm_use_im_patch_token: False
model.config.tokenizer_padding_side: right
model.config.image_aspect_ratio: square
model_args.version: plain
tokenizer.pad_token: <unk>
mm_patch_merge_type: flat
 model.config.mm_use_im_start_end: False
 model.config.mm_projector_lr: None
training_args.use_im_start_end: False
model.config.tokenizer_model_max_length: 2048
model.config.mm_use_im_patch_token: False
model.config.tokenizer_padding_side: right
model.config.image_aspect_ratio: square
model_args.version: plain
tokenizer.pad_token: <unk>
mm_patch_merge_type: flat
 model.config.mm_use_im_start_end: False
 model.config.mm_projector_lr: None
training_args.use_im_start_end: False
model.config.tokenizer_model_max_length: 2048
model.config.mm_use_im_patch_token: False
model.config.tokenizer_padding_side: right
model.config.image_aspect_ratio: square
model_args.version: plain
tokenizer.pad_token: <unk>
mm_patch_merge_type: flat
 model.config.mm_use_im_start_end: False
 model.config.mm_projector_lr: None
training_args.use_im_start_end: False
model.config.tokenizer_model_max_length: 2048
model.config.mm_use_im_patch_token: False
model.config.tokenizer_padding_side: right
model.config.image_aspect_ratio: square
model_args.version: plain
tokenizer.pad_token: <unk>
mm_patch_merge_type: flat
Formatting inputs...Skip in lazy mode
{'loss': 8.7674, 'learning_rate': 5e-06, 'epoch': 0.0}
{'loss': 8.6845, 'learning_rate': 1e-05, 'epoch': 0.0}
{'loss': 9.0315, 'learning_rate': 1.5e-05, 'epoch': 0.0}
{'loss': 9.0582, 'learning_rate': 2e-05, 'epoch': 0.0}
{'loss': 9.0464, 'learning_rate': 2.5e-05, 'epoch': 0.0}
{'loss': 8.6453, 'learning_rate': 3e-05, 'epoch': 0.0}
{'loss': 10.3615, 'learning_rate': 3.5000000000000004e-05, 'epoch': 0.0}
{'loss': 10.8273, 'learning_rate': 4e-05, 'epoch': 0.0}
{'loss': 10.2301, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.0}
{'loss': 8.8912, 'learning_rate': 5e-05, 'epoch': 0.0}
{'train_runtime': 16.4226, 'train_samples_per_second': 4.871, 'train_steps_per_second': 0.609, 'train_loss': 9.354344654083253, 'epoch': 0.0}
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /home/aac/ssrivas9/Layer_Select_Fuse_for_MLLM/wandb/offline-run-20250405_022816-tnrjn5fh[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250405_022816-tnrjn5fh/logs[0m
[2025-04-05 02:29:13,389] [INFO] [launch.py:351:main] Process 249619 exits successfully.
[2025-04-05 02:29:13,389] [INFO] [launch.py:351:main] Process 249616 exits successfully.
[2025-04-05 02:29:13,390] [INFO] [launch.py:351:main] Process 249618 exits successfully.
[2025-04-05 02:29:13,391] [INFO] [launch.py:351:main] Process 249617 exits successfully.
[2025-04-05 02:29:14,392] [INFO] [launch.py:351:main] Process 249615 exits successfully.
[2025-04-05 02:29:14,393] [INFO] [launch.py:351:main] Process 249613 exits successfully.
[2025-04-05 02:29:14,394] [INFO] [launch.py:351:main] Process 249614 exits successfully.
[2025-04-05 02:29:15,396] [INFO] [launch.py:351:main] Process 249612 exits successfully.

